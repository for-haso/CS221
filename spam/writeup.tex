\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}

\begin{document}

\begin{center}
{\Large CS221 Fall 2013 Homework: Spam}

\begin{tabular}{rl}
SUNet ID: & nisham \\
Name: & Nisha Masharani \\
Collaborators: & sryoung, alesan92
\end{tabular}
\end{center}

By turning in this assignment, I agree by the Stanford honor code and declare
that all of this is my own work.

\section*{Problem 1.1}

\begin{enumerate}[label=(\alph*)]
  \item See code for part A.

  \item 
	  \begin{tabular}{ l | p{3cm} | p{3cm} | p{3cm} }
	    \hline
	     & 1 & 2 & 3\\ \hline
	    10000 & train: 0.114308, dev: 0.105919 & train: 0.167631, dev: 0.165109 & train: 0.215168, dev: 0.204361 \\ \hline
	    20000 & train: 0.168100, dev: 0.163863 & train: 0.114464, dev: 0.118380 & train: 0.106646, dev: 0.106542 \\ \hline
	    30000 & train: 0.482252, dev: 0.489720 & train: 0.470993, dev: 0.477882 & train: 0.457076, dev: 0.464174 \\ \hline
	  \end{tabular}
\end{enumerate}

\section*{Problem 1.2}

\begin{enumerate}[label=(\alph*)]
  \item See code for part A.
  \item See code for part B.
  \item Let $w$ be a vector containing a score of $1.0$ for the first $k$ words in the blacklist, and a score of $0.0$ for all words not in the blacklist. Then $w \cdot \phi(x)$ ends up being equal to the count of all blacklisted words in the email. For example, if the blacklist contains the words "hello" and "goodbye", then $w$ = {"hello": 1.0, "goodbye": 1.0}. If I have an email that reads "hello my name is joe.", $\phi(x)$ = {"hello": 1.0, "my": 1.0, "name": 1.0, "is": 1.0, "joe": 1.0}. $w \cdot \phi(x)$ will then be 1.0, which is consistent with the score from the rule-based classifier. We can write the mathematical output of the classifier as follows:

  $\hat y = \left\{ \begin{matrix} 
	1 ~\textrm{if}~ w \cdot \phi(x) - n \ge 0\\
	-1 ~\textrm{if}~ w \cdot \phi(x) < 0
	\end{matrix}\right..$
\end{enumerate}

\section*{Problem 1.3}

\begin{enumerate}[label=(\alph*)]
  \item See code for part A.
  \item See code for part B.

  \item TODO
  \item 
  \begin{tabular}{ l | p{3cm} | p{3cm} }
	    \hline
	    Num examples & trainErrorRate & devErrorRate\\ \hline
	    500 & 0.079281 & 0.087227 \\ \hline
	    1000 & 0.053792 & 0.067290 \\ \hline
	    1500 & 0.039406 & 0.057321 \\ \hline
	    2000 & 0.028147 & 0.047352 \\ \hline
	    2500 & 0.021423 & 0.040498 \\ \hline
	    3000 & 0.021423 & 0.039875 \\ \hline
	    3500 & 0.015794 & 0.038629 \\ \hline
	    4000 & 0.015481 & 0.039252 \\ \hline
	    4500 & 0.010633 & 0.033645 \\ \hline
	    5000 & 0.009851 & 0.038006 \\ \hline
	  \end{tabular}
\end{enumerate}

\section*{Problem 2}

\begin{enumerate}[label=(\alph*)]
  \item 
  Unigram:
  trainErrorRate: 0.020681
  devErrorRate: 0.185393
  Bigram:
  trainErrorRate: 0.493917
  devErrorRate: 0.505618

  TODO: this is confusing
  \item
  \begin{tabular}{ l | p{3cm} | p{3cm} }
    \hline
    Num iters & trainErrorRate & devErrorRate\\ \hline
    1 & 0.422141 & 0.466292 \\ \hline
    2 & 0.464720 & 0.494382 \\ \hline
    3 & 0.126521 & 0.292135 \\ \hline
    4 & 0.502433 & 0.511236 \\ \hline
    5 & 0.091241 & 0.292135 \\ \hline
    6 & 0.023114 & 0.174157 \\ \hline
    7 & 0.492701 & 0.505618 \\ \hline
    8 & 0.013382 & 0.157303 \\ \hline
    9 & 0.069343 & 0.207865 \\ \hline
    10 & 0.008516 & 0.162921 \\ \hline
    11 & 0.006083 & 0.179775 \\ \hline
    12 & 0.029197 & 0.235955 \\ \hline
    13 & 0.013382 & 0.151685 \\ \hline
    14 & 0.004866 & 0.191011 \\ \hline
    15 & 0.013382 & 0.174157 \\ \hline
    16 & 0.006083 & 0.168539 \\ \hline
    17 & 0.004866 & 0.140449 \\ \hline
    18 & 0.009732 & 0.157303 \\ \hline
    19 & 0.003650 & 0.146067 \\ \hline
    20 & 0.493917 & 0.505618 \\ \hline
  \end{tabular}

    Dev error rate does increase monotonically with training error rate. This makes sense because, as the training error rate decreases, the model ends up fitting the training data. If the training data is similar to the dev data, and the model has not been over-fitted to the training data, the dev error rate should decrease as well. If the model has been over-fitted to the training data (which we do not see here), we would see the dev error rate start to increase as the training error rate gets really small. 

\end{enumerate}

\section*{Problem 3}

\begin{enumerate}[label=(\alph*)]
  \item See code for part A.

  \item 
  Bigram:\\
  trainErrorRate: 0.000000\\
	devErrorRate: 0.105919\\
	Unigram:\\
	trainErrorRate: 0.008444\\
	devErrorRate: 0.128349
\item TODO
\end{enumerate}

\end{document}
