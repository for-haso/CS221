<head>
  <script src="plugins/main.js"></script>
  <title>Text reconstruction</title>

  <style type="text/css">
    .nl { font-family:monospace; }
  </style>
</head>

<script type="text/javascript">
  function _getLink() { return '<a href="http://ai.stanford.edu/~rfrostig/">Roy Frostig</a>' }
</script>
<body onload="onLoad('Roy', 'Assignment by ' + _getLink())">

<div id="assignmentHeader"></div>

<p><img src="holykeys.png"/></p>

<p>
  Plenty of human knowledge is stored and communicated in text, so
  designing artificially intelligent systems often requires some form
  of natural language processing (NLP).  The purpose of this
  assignment is to tackle two building-block tasks in text processing,
  first separately and then assembled together as a joint task.
</p>
<p>
  The two tasks we'll consider are <i>word segmentation</i>
  and <i>vowel insertion</i>.

  Word segmentation often comes up in processing many non-English
  languages, in which words might not be flanked by spaces on either
  end, such as in written Chinese or in long compound German
  words.<sup><a href="#fn-1">[1]</a></sup>

  Vowel insertion is relevant in languages such as Arabic or Hebrew,
  for example, where modern script eschews notations for vowel sounds
  and the human reader infers them from
  context.<sup><a href="#fn-2">[2]</a></sup> More generally, it is an
  instance of a reconstruction problem given lossy encoding and some
  context.
</p>
<p>
  We already know how to optimally solve any particular min-cost
  state-space search problems with graph search algorithms such as
  uniform cost search or A*.  Our purpose with this assignment is to
  exercise how to model engineering tasks (such as our two NLP tasks)
  as such state-space search problems.
</p>

<!------------------------------------------------------------>
<div class="problemTitle">Setup: $n$-gram language models and
uniform-cost search</div>

<p>
  Our algorithm will base segmentation and insertion decisions based
  on the cost of produced text according to a <i>language model</i>.
  A language model is some function of the processed text that
  captures its fluency.
</p>

<p>
  A very common language model in NLP is an $n$-gram sequence model: a
  function that, given $n$ consecutive words, gives a score based on
  to the likelihood that the $n$th word appears just after the first
  $n-1$.<sup><a href="#fn-3">[3]</a></sup>

  In our case, this score will be interpreted as a cost.  It will
  always be positive, and lower costs indicate better
  fluency.<sup><a href="#fn-4">[4]</a></sup>
  As a simple example: in a case where $n=2$ and $\ell(\cdot)$ is our
  $n$-gram cost function, $\ell(\mathsf{big}, \mathsf{fish})$ would be
  low, but $\ell(\mathsf{fish}, \mathsf{fish})$ would be fairly high.
</p>
<p>
  Furthermore, these costs are additive: for any $n > 0$, if we have
  observed the word sequence $w_1, \ldots, w_{n-1}$, then the cost of
  succeeding it with the word sequence $w_n, w_{n+1}$ is given by \[
    \ell(w_1, w_2, \ldots, w_n) + \ell(w_2, w_3, \ldots, w_n, w_{n+1}).
  \]

  To keep things simple, we'll consider $n = 1$ (unigram cost
  function) and $n = 2$ (bigram cost function).  Note that a unigram
  cost function simply scores individual words based on their frequency in
  naturally-occurring text.
</p>

<p>
  A note on low-level efficiency and expectations: this assignment was
  designed considering input sequences of length no greater than
  roughly 200 (characters, or list items, depending on the task).  Of
  course, it's great if programs tractably manage larger inputs,
  but it isn't expected that such inputs not lead to inefficiency
  due to overwhelming state space growth.
</p>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 1: word segmentation</div>

<p>
  In word segmentation, you are given as input a string of
  alphabetical characters (<code>[a-z]</code>) without whitespace, and
  your goal is to insert spaces into this string such that the result
  is the most fluent according to the language model.
</p>

<ol class="problem">

<li class="writeup">
  <p>
    As a warmup: You are given a unigram cost function $u(w_1)$ and a
    bigram function $b(w_1, w_2)$, where the $w_i$ represent words in
    sequential order.  What is the total cost of a segmented sentence
    $w_1, w_2, \ldots, w_n$ under the unigram cost function alone?
    And under the bigram cost function?
  </p>
  <p>
    When handling a bigram cost function, you may assume a special
    out-of-vocabulary word <code>-BEGIN-</code> that corresponds to
    the beginning of a sentence, so $b($ <code>-BEGIN-</code> $, w_1)$
    is the cost of $w_1$ appearing at the front of a sentence.
  </p>

</li>

<li class="writeup">
  <p>
    A reasonable first approach might be to use a greedy algorithm.
    Propose a greedy algorithm that runs in time $O(n^2)$ where $n$ is
    the input sequence character length.
  </p>
</li>

<li class="writeup">
  <p>
    Now show that this greedy search is suboptimal.  In particular,
    provide an example input string on which the greedy approach would
    fail to find the lowest-cost segmentation of the input.
  </p>
  <p>
    In creating this example, you are free to design the $n$-gram cost
    function (both the choice of $n$ and the cost of any $n$-gram
    sequences) but costs must be positive and lower cost should
    indicate better fluency.  Your example should be based on a
    realistic English word sequence &mdash; don't simply use abstract
    symbols with designated costs.
  </p>
</li>

<li class="code">
  <p>
    Implement an algorithm that, unlike greedy, finds the optimal word
    segmentation of an input character sequence.  Your algorithm
    will consider costs based simply on a unigram cost function.
  </p>
  <p>
    Before jumping into code, you should think about how to frame
    this problem as a state-space search problem.  How would you
    represent a state?  What are the successors of a state?  What are
    the state transition costs?  (You don't need to answer these
    questions in your writeup.)
  </p>
  <p>
    As in the <a href="../delivery/">delivery</a> assignment, uniform
    cost search (UCS) is implemented for you, and you should make use of
    it here.<sup><a href="#fn-5">[5]</a></sup>
  </p>
  <p>
    Fill in the member functions of
    the <code>SegmentationProblem</code> class and
    the <code>segmentWords</code> function.

    The argument <code>unigramCost</code> is a function that takes in
    a single string representing a word and outputs its unigram cost.

    The function <code>segmentWords</code> should return the segmented
    sentence with spaces as delimiters, i.e. <code>' '.join(words)</code>.
  </p>
  <p>
    For convenience, you can actually run <code>python
    submission.py</code> to enter a console in which you can type
    character sequences that will be segmented by your implementation
    of <code>segmentWords</code>.  To request a segmentation,
    type <code>seg mystring</code> into the prompt.  For example:
    <pre>
      >> seg thisisnotmybeautifulhouse

        Query (seg): thisisnotmybeautifulhouse

        this is not my beautiful house
    </pre>
    Console commands other than <code>seg</code> &mdash;
    namely <code>ins</code> and <code>both</code> &mdash; will be used for
    the upcoming parts of the assignment.  Other commands that might
    help with debugging can be found by typing <code>help</code> at
    the prompt.
  </p>
</li>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 2: vowel insertion</div>

<p>
  Now you are given a sequence of English words with their vowels
  missing (A, E, I, O, and U; never Y).  Your task is to place vowels
  back into these words in a way that maximizes sentence fluency
  (i.e., that minimizes sentence cost).  For this task, you will use a
  bigram cost function.
</p>
<p>
  You are also given a mapping <code>possibleFills</code> that maps
  any vowel-free word to a set of possible reconstructions (complete
  words).<sup><a href="#fn-6">[6]</a></sup> For
  example, <code>possibleFills('fg')</code>
  returns <code>set(['fugue', 'fog'])</code>.
</p>

<ol class="problem">

<li class="writeup">
  <p>
    Propose a greedy algorithm for this task.  Your algorithm should
    run in time $O(km)$ where $m$ is the number of vowel-free words in
    the input sequence and $k$ is the largest number of
    reconstructions that <code>possibleFills</code> might provide for
    a vowel-free word.
  </p>
</li>

<li class="writeup">
  <p>
    Show, as in question 1, that this greedy algorithm is suboptimal,
    by providing a realistic counter-example using English text.  Make
    any assumptions you'd like about <code>possibleFills</code> and
    the bigram cost function, but bigram costs must remain positive.
  </p>
</li>

<li class="code">
  <p>
    Implement an algorithm that finds optimal vowel insertions.  Use
    the UCS subroutines.
  </p>
  <p>
    When you've completed your implementation, the
    function <code>insertVowels</code> should return the reconstructed
    word sequence as a string with space delimiters, i.e.
    <code>' '.join(filledWords)</code>.
  </p>
  <p>
    The argument <code>queryWords</code> is the input sequence of
    vowel-free words.  Note well that the empty string is a valid such
    word.  The argument <code>bigramCost</code> is a function that
    takes two strings representing two sequential words and provides
    their bigram score.  The special out-of-vocabulary
    beginning-of-sentence word <code>-BEGIN-</code> is given
    by <code>wordsegUtil.SENTENCE_BEGIN</code>.  The
    argument <code>possibleFills</code> is a function; it takes a word
    as string and returns a <code>set</code> of
    reconstructions.
  </p>
  <p>
    <b>NB:</b> If some vowel-free word $w$
    has no reconstructions according to <code>possibleFills</code>,
    your implementation should consider $w$ itself as the sole
    possible reconstruction.
  </p>
  <p>
    Use the <code>ins</code> command in the program console to try
    your implementation.  For example:
    <pre>
      >> ins thts m n th crnr

        Query (ins): thts m n th crnr

        thats me in the corner
    </pre>
    The console strips away any vowels you do insert, so you can
    actually type in plain English and the vowel-free query will be
    issued to your program.  This also means that you can use a single
    vowel letter as a means to place an empty string in the sequence.
    For example:
    <pre>
      >> ins its a beautiful day in the neighborhood

        Query (ins): ts  btfl dy n th nghbrhd

        its a beautiful day in the neighborhood
    </pre>
  </p>
</li>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 3: joint problem-solving</div>

<p>
  We'll now see that it's possible to solve both of these tasks at
  once.  This time, you are given a whitespace- and vowel-free string
  of alphabetical characters.  Your goal is to insert spaces and
  vowels into this string such that the result is the most fluent
  possible one.  As in the previous task, costs are based on a bigram
  cost function.
</p>

<ol class="problem">

<li class="code">
  <p>
    Implement an algorithm that finds the optimal space and
    vowel insertions.  Use the UCS subroutines.
  </p>
  <p>
    When you've completed your implementation, the
    function <code>segmentAndInsert</code> should return a segmented
    and reconstructed word sequence as a string with space delimiters,
    i.e. <code>' '.join(filledWords)</code>.
  </p>
  <p>
    The argument <code>query</code> is the input string of space- and
    vowel-free words.  The argument <code>bigramCost</code> is a
    function that takes two strings representing two sequential words
    and provides their bigram score.  The special out-of-vocabulary
    beginning-of-sentence word <code>-BEGIN-</code> is given
    by <code>wordsegUtil.SENTENCE_BEGIN</code>.  The
    argument <code>possibleFills</code> is a function; it takes a word
    as string and returns a <code>set</code> of reconstructions.
  </p>
  <p>
    <b>NB:</b> Unlike in problem 2, where a vowel-free word
    could (under certain circumstances) be considered a valid
    reconstruction of itself, here you should never include in your
    output a word that is not the reconstruction of some vowel-free
    word according to <code>possibleFills</code>.
  </p>
  <p>
    Use the command <code>both</code> in the program console to try
    your implementation.  For example:
    <pre>
      >> both mgnllthppl

        Query (both): mgnllthppl

        imagine all the people
    </pre>
  </p>
</li>

</ol>

<hr/>
<p id="fn-1"> [1]
  In German, <i>Windschutzscheibenwischer</i> is "windshield wiper".
  Broken into parts: <i>wind</i> ~ wind; <i>schutz</i> ~ block /
  protection; <i>scheiben</i> ~ panes; <i>wischer</i> ~ wiper.
</p>
<p id="fn-2"> [2]
  See <a href="https://en.wikipedia.org/wiki/Abjad">https://en.wikipedia.org/wiki/Abjad</a>.
</p>
<p id="fn-3"> [3]
  This model works under the assumption that text roughly satisfies
  the <a href="https://en.wikipedia.org/wiki/Markov_property">Markov
  property</a>.
</p>
<p id="fn-4"> [4]
  Modulo edge cases, the $n$-gram model score in this assignment is
  given by $\ell(w_1, \ldots, w_n) = -\log(p(w_n \mid w_1, \ldots,
  w_{n-1}))$.  Here, $p(\cdot)$ is an estimate of the conditional
  probability distribution over words given the sequence of previous
  $n-1$ words.  This estimate is gathered from frequency counts taken
  by reading Leo Tolstoy's <i>War and Peace</i> and William
  Shakespeare's <i>Romeo and Juliet</i>.
</p>
<p id="fn-5"> [5]
  Solutions that use UCS ought to exhibit fairly fast execution time
  for this problem, so using A* here is unnecessary.
</p>
<p id="fn-6"> [6]
  This mapping, too, was obtained by reading Tolstoy and Shakespeare
  and removing vowels.
</p>

</body>
