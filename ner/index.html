<head>
  <title>Named-entity recognition</title>

  <script src="plugins/sfig/internal/sfig.js"></script>
  <script src="plugins/utils.js"></script>
  <script src="plugins/main.js"></script>
  <script src="index.js"></script>

  <style type="text/css">
    .diagram{
      text-align: center;
      margin-top: 20px;
      margin-bottom: 20px;
      margin-left : auto;
      margin-right : auto;
      width: 80%;
    }
  </style>

</head>

<body onload="onLoad('Arun Chaganty'); renderFigures();">
<!------------------------------------------------------------>

<div id="assignmentHeader"></div>
<p>
  <span style="color:red;">Last Update: 11/06/2013 9:40 PM</span>
</p>

<p>
One of the principal aims of natural language processing is to build a system
that can automatically read a piece of text and determine who is doing what to whom.
A first step towards that goal is named-entity recognition, the task
of taking a piece of text and tagging each word as either person, organization, location, or none of the above.
Here's an example:
</p>
<blockquote>
<div class="diagram" id="ner-example"></div>
</blockquote>

<p>
In this assignment, we will build a named-entity recognition
system using factor graphs.
We will start with a chain-structured factor graph called a linear-chain conditional random field (CRF),
which admits exact inference via variable elimination.
Then, we will develop a more sophisticated factor graph to capture long-range dependencies which are common
in natural language.  For this model, we will use Gibbs sampling to perform approximate inference.
</p>

$$
\require{color}
\newcommand{\wt}{\theta}
\newcommand{\BEGIN}{{\color{green}\textrm{-BEGIN-}}}
\newcommand{\END}{{\color{green}\textrm{-END-}}}
\newcommand{\FEAT}{{\color{green}\textrm{-FEAT-}}}
\newcommand{\SIZE}{{\color{green}\textrm{-SIZE-}}}
\newcommand{\Viterbi}{\textrm{Viterbi}}
\newcommand{\Suffix}{\textrm{Suffix}}
\newcommand{\IsCapitalized}{\textrm{IsCapitalized}}
\newcommand{\Forward}{\textrm{Forward}}
\newcommand{\Backward}{\textrm{Backward}}
\newcommand{\mI}{\mathbb{I}}
\newcommand{\xseq}{\mathbf{x}}
\newcommand{\yseq}{\mathbf{y}}
\newcommand{\philocal}{\phi_\text{local}}
$$

<h3>Setup: Linear-chain conditional random fields (CRFs)</h3>
<p>
Let $\xseq = (x_1, \dots, x_T)$ be a sequence of words,
and let $\yseq = (y_1, \dots, y_T)$ represent a sequence of tags.
We will model the NER task with the following simple factor graph:
<div class="diagram" id="linear-chain-crf"> </div>
Here, the tags $\yseq$ are the only variables; the sequence of words $\xseq$
only affects the potentials $G_t$, as we'll see later.
The probability of a tag sequence $\yseq$ given $\xseq$ is:
\begin{align}
p(\yseq \mid \xseq; \theta) &= \frac{1}{Z(\xseq; \theta)} \prod_{t=1}^T G_t( y_{t-1}, y_t; \xseq, \theta ) \\
Z(\xseq; \theta) &= \sum_{\yseq} \prod_{t=1}^T G_t( y_{t-1}, y_t; \xseq, \theta ), 
\end{align}
where $y_0 =\BEGIN$ and $Z(\xseq; \theta)$ is the normalization constant.
The potentials are:
$$G_t(y_{t-1}, y_t; \xseq, \theta) = \exp( \wt \cdot \philocal(t, y_{t-1}, y_t, \xseq) ),$$ where $\philocal$ is the local
feature function and $\wt \in \mathbb R^d$ is the parameter vector.
Note that the $\philocal$ can depend arbitrarily on the input $\xseq$,
and will generally access the words around position $t$ (e.g., $x_{t-1}$, $x_t$, $x_{t+1}$).
</p>

<p>
We have provided you with the function <code>LinearChainCRF.G(t, y_, y,
  xs)</code> to compute the value $G_t(y_{t-1}, y_t ; \xseq, \theta)$,
where <code>y_</code> is $y_{t-1}$, <code>y</code> is $y_t$ and <code>xs</code> is $\xseq$.
Note that in math indexing starts from 1 (e.g., $y_1$ is the first tag),
but in code, indexing starts from 0 (e.g., <code>ys[0]</code> is the first tag).
To get the value $G_3(y_2, y_3 ; \xseq, \theta)$, call <code>LinearChainCRF.G(2, ys[1], ys[2], xs)</code>, where
<code>ys</code> is the tag sequence $\yseq$ and <code>xs</code> is the
observation sequence $\xseq$.
For $y_{0} = \BEGIN$, use the provided constant <code>BEGIN_TAG</code>; for example
$G_1(\BEGIN, y_1 ; \xseq, \theta)$ is
<code>G(0, BEGIN_TAG, ys[0], xs)</code>.
</p>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 1: Inference in linear-chain CRFs</div>

<ol class="problem">

  <li class="code"> <strong>(5 points) Viterbi algorithm.</strong>
    <p>
    Our first task is to compute the best tag sequence $\yseq^*$
    given a sentence $\xseq$ and a fixed parameter vector $\theta$:
    \begin{align}
    \yseq^* &= \arg\max_{\yseq} p(\yseq \mid \xseq; \theta) \\
        &= \arg\max_{\yseq} \prod_{t=1}^T G_t( y_{t-1}, y_t; \xseq, \theta ).
    \end{align}
    </p>

    <p>
    Recall from class that the Viterbi algorithm eliminates the variables
    $y_1, y_2, \dots, y_T$ from left to right, producing a sequence of max forward messages $\Viterbi_1, \dots, \Viterbi_T$.
    Eliminating $y_0$ produces a unary potential:
    <div class="diagram" id="viterbi-crf-1"></div>
    \begin{align}
    \Viterbi_1(y_1) &= G_1({\color{green}\textrm{-BEGIN-}}, y_1; \xseq, \theta).
    \end{align}
    Eliminating $y_1$ yields:
    <div class="diagram" id="viterbi-crf-2"></div>
    \begin{align}
    \Viterbi_2(y_2) &= \max_{y_1} \Viterbi_1(y_1) G_2(y_1, y_2; \xseq, \theta).
    \end{align}
    (Note that the notation differs slightly from class,
    where the max forward messages $F_i$ coresponds to eliminating variables up to and including $y_i$, whereas
    $\Viterbi_i$ corresponds to eliminating variables up to but excluding $y_i$.)
    </p>

    <p>
    Repeating this process gives us the following algorithm.
    <ol>
      <li>Initialize $\Viterbi_0(y_0) = 1$.</li>
      <li>For $t = 1, \dots, T$, compute $\Viterbi_t(y_t) = \max_{y_{t-1}} \Viterbi_{t-1}(y_{t-1}) G_t(y_{t-1}, y_t ; \xseq, \theta)$.</li>
      <li>Return the maximum weight $\max_{y_T} \Viterbi_T(y_T)$.</li>
    </ol>
    </p>

    <p>
    In order to recover the actual sequence $\yseq^*$, we can work backwards
    from the value of $y_T$ that maximizes $\Viterbi_T$, back to the
    optimal assignment of $y_1$.
    <ol>
      <li>Compute $y^*_T = \arg\max_{y_T} \Viterbi_T(y_T)$.</li>
      <li>For $t = T, \dots, 2$, compute $y^*_{t-1} = \arg\max_{y_{t-1}} \Viterbi_{t-1}(y_{t-1}) G_t(y_{t-1}, y^*_t ; \xseq, \theta )$.</li>
    </ol>
    </p>

    <p>
    <b>Implement <code>computeViterbi</code>.</b>
    </p>

    <p>
    Once you have implemented <code>computeViterbi</code>, you can run the following command to get an interactive shell to play around with your CRF.
    <pre>
$ python run.py shell --parameters data/english.binary.crf --featureFunction binary
&gt;&gt; viterbi Mark had worked at Reuters
-PER- -O- -O- -O- -ORG-
    </pre>
    </p>
    <p>
    Use <code>python run.py --help</code> for more options and type <code>help</code> in the interpreter for more information.
    </p>
    </li>

  <li class="code">
    <strong>(10 points) Computing forward and backward messages.</strong>
  <p>
  Next, let us compute the forward and backward messages.
  Using the standard approach from class will lead to numerical underflow/overflow errors,
  so we normalize the $\Forward_t$ messages at each step and keep track of the log normalization constant $A$.
  </p>

  <p>
    <ol>
      <li> Initialize $\Forward_0(y_0) = 1$ and $A = 0$.</li>
      <li> For $t = 1, \dots, T$: 
      <ol>
        <li> Compute $\Forward_t(y_t) = \sum_{y_{t-1}} \Forward_{t-1}(y_{t-1}) G_t(y_{t-1}, y_t; \xseq, \theta)$.</li>
        <li> Update $A \gets A + \log\left( \sum_{v} \Forward_t(v) \right)$.</li>
        <li> Normalize: $\Forward_t(y_t) \gets \frac{\Forward_t(y_t)}{\sum_{v} \Forward_t(v)}$.</li>
      </ol>
      </li>
      <li>Return $\exp(A)$, which equals the normalization constant $Z(\xseq; \theta)$.</li>
    </ol>
  </p>

  <p>
    <b>
      Implement <code>computeForward</code>, which returns
      the log normalization constant $A$ and the (normalized) forward messages $[\Forward_1(y), \dots, \Forward_T(y)]$.
    </b> 
    </p>
  </li>

  <p>
    We could also compute $Z(\xseq; \theta)$ by eliminating variables from right to left
    ($y_T, y_{T-1}, \dots, y_1$), which would produce a sequence of 
    backward potentials $\Backward_t(y_t)$.  We have provided <code>computeBackward</code> which returns the normalized backward messages $[\Backward_1(y), \dots, \Backward_T(y)]$.
  </p>

  <li class="code">
    <strong>(5 points) Computing marginal probabilities.</strong>
  <p>
  Given the forward and backward messages, we can combine them to compute
  marginal probabilities:
  \begin{align}
  p(y_{t-1}, y_{t} \mid \xseq; \theta) &= \frac{\Forward_{t-1}(y_{t-1}) G_t(y_{t-1}, y_{t}; \xseq, \theta) \Backward_{t}(y_{t})}{Z(\xseq; \theta)}.
  \end{align}
  </p>

  <p>
  <b>
    Implement <code>computeEdgeMarginals</code> that will compute $p(y_{t-1}, y_{t} \mid \xseq; \theta)$.
    You should use <code>computeForward</code> and <code>computeBackward</code>. 
  </b>
  </p>

  <p>
  We have implemented the learning algorithm that uses these marginals to compute a gradient.
  You can train the CRF (with standard features explained in problem 2) now by running:
<pre>
$ python run.py train --featureFunction binary --output-path my.crf
</pre>
  It could take up to 10-20 minutes to train, so only do this after you're confident that your code works.
  The program will write the parameters of the trained CRF to <code>my.crf</code>.
  You should get a dev F1 score of around 56.7%, which is quite poor.
  In the next section, we will design better features that will substantially improve the accuracy.
  </p>
  </li>
</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 2: Named-entity recognition</div>

<p>
In the previous problem, we developed all the algorithms required
to train and use a linear-chain CRF.  Now we turn to designing better features.
</p>

<p>
We are using a subset of the <a href="http://www.cnts.ua.ac.be/conll2003/ner/">CoNLL 2003 dataset</a>
consisting of 2,000 sentences with the following NER tags:
<code>-PER-</code> (person), <code>-LOC-</code> (location), <code>-ORG-</code> (organization),
<code>-MISC-</code> (miscellaneous), and <code>-O-</code> (other).
</p>

<p>
We have provided two very simple feature functions. We will describe the feature functions
using the example three-word sentence:
<pre>xs = ["Beautiful", "2", "bedroom"]</pre>
<ol>
  <li> <code>unaryFeatureFunction(t, y_, y, xs)</code> introduces an
  indicator feature for the current tag <code>y</code> and the current word <code>xs[t]</code>.
  For example, <code>unaryFeatureFunction(2, "-FEAT-", "-SIZE-", xs)</code> would return
  <pre>
  { ("-SIZE-", "bedroom") : 1.0 }.
  </pre>
  </li>
  <li> <code>binaryFeatureFunction(t, y_, y, xs)</code> includes all
  the features from <code>unaryFeatureFunction</code> and introduces another
  indicator feature for the previous tag <code>y_</code> and the current tag <code>y</code>. 
  For example, <code>binaryFeatureFunction(2, "-FEAT-", "-SIZE-", xs)</code> would return,
  <pre> { ("-FEAT-", "-SIZE-") : 1.0, ("-SIZE-", "bedroom") : 1.0 }.</pre>
  </li>
</ol>
</p>

<p>
To train a model, use the following command, 
<pre>
$ python run.py train --featureFunction [unary|binary|ner|betterNer] --output-path my.crf
</pre>
As the model trains, it will print the likelihood of the training data
at each iteration as well as
a <a href="http://en.wikipedia.org/wiki/Confusion_matrix">confusion
matrix</a> and <a href="http://en.wikipedia.org/wiki/F1_score">F1
score</a> for the NER tags. If you specify an output path, you can
interact with the CRF you trained by providing the path as an argument
to the shell,
<pre>
  $ python run.py shell --parameters my.crf --featureFunction [unary|binary|ner|betterNer]
</pre>
Remember to use the same feature function as the one you used to train!
Again, use <code>python run.py --help</code> for more options.
</p>

<ol class="problem">

<li class="code">
<strong>(5 points) Feature extraction.</strong>

<p>
A common problem in NLP is making accurate predictions for words that we've never seen during training
(e.g., <i>Punahou</i>).  The reason why our accuracy is so low is that all our
features thus far are defined on entire words, whereas we'd really like to
generalize.  Fortunately, a CRF allows us to
us to choose arbitrary feature functions $\philocal(t, y_{t-1}, y_t,
\xseq)$.
Next, we will define features based on capitalization or suffixes, which allow us to generalize
to unseen words, as well as features that look at the current tag and the previous and next words,
to capture more context.
</p>

<p>
<b>Implement <code>nerFeatureFunction(t, y_, y, xs)</code> with the features below.</b>
Again, we will illustrate the expected output using the sentence:
<pre>xs = ["Beautiful", "2", "bedroom"]</pre>
Note: for convenience, think of the sentence as being padded with special begin/end words:
<code>xs</code> is <code>-BEGIN-</code> at position <code>-1</code>
and <code>-END-</code> at position <code>len(xs)</code>
<ul>
  <li>All the features from <code>binaryFeatureFunction</code>.</li>
  <li>An indicator feature on the current tag <code>y</code> and the capitalization of the current word <code>xs[t]</code>. For
  example, <code>nerFeatureFunction(0, "-BEGIN-", "-FEAT-", xs)</code> would include the following features,
  <pre>{ ("-FEAT-", "-CAPITALIZED-") : 1.0 }.</pre>
  On the other hand, <code>nerFeatureFunction(2, "-SIZE-", "-SIZE-", xs)</code> would
  not add any features because "bedroom" is not capitalized.
  </li>
  <li>An indicator feature on the current tag <code>y</code> and the
  <em>previous word</em> <code>xs[t-1]</code>.
  For example, <code>nerFeatureFunction(2, "-SIZE-", "-SIZE-", xs)</code> would add:
<pre>{ ("-SIZE-", "PREV:2") : 1.0 }.</pre>
  And <code>nerFeatureFunction(0, "-BEGIN-", "-FEAT-", xs)</code> would add
<pre>{("-FEAT-", "PREV:-BEGIN-") : 1.0}.</pre>
  </li>
  <li>A similar indicator feature for the current tag <code>y</code> and
  <em>next</em> word <code>xs[t+1]</code>;
  For example, <code>nerFeatureFunction(0, "-BEGIN-", "-FEAT-", xs)</code> would include, 
<pre>{("-FEAT-", "NEXT:2") : 1.0},</pre>
  and <code>nerFeatureFunction(2, "-SIZE-", "-SIZE-", xs)</code> would include,
<pre>{("-SIZE-", "NEXT:-END-") : 1.0} .</pre>
  </li>
  <li>Repeat the above two features except using capitalization instead
  of the actual word; consider <code>-BEGIN-</code> and
  <code>-END-</code> to be un-capitalized.  An an example,
  <code>nerFeatureFunction(1, "-SIZE-", "-SIZE-", ["Beautiful", "2", "Bedroom"])</code> would include
<pre>{("-SIZE-", "-PRE-CAPITALIZED-") : 1.0, ("-SIZE-", "-POST-CAPITALIZED-") : 1.0}.</pre>
  </li>
</ul>
</p>
</li>

After you verify that your features are working using <code>grader.py</code>,
train your CRF:
<pre>
$ python run.py train --featureFunction ner --output-path ner.crf
</pre>
Your dev F1 score should be around 71%.
</li>

<li class="code">
<strong>(10 points <u>extra credit</u>) Competition.</strong> 
<p>
<b>Create your own features to improve accuracy on the CoNLL 2003 dataset.</b> You will be
evaluated on an unseen test set. The top 5 submissions
will be assigned full credit, and the remainder will be assigned credit
proportional to how well they compare with the winners.
</p>
<p>
<b>To be eligible for extra credit, you must also submit a writeup
describing the features you implemented for the competition.</b>
</p>

</li>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 3: Handling long-range dependencies</div>

<p>
Consider the following example,
<div class="diagram" id="long-range-crf"></div>
It is clear that in the first occurrence of the word, "Tanjung" is an
organization. Given only the second sentence though, it is ambiguous
whether "Tanjung" is a person or organization.  In fact, the CRF we
previously trained predicts "Tanjung" to be a person here. To address this,
we would like to add a constraint that all occurrences of a word are tagged the same.
The catch is that such a constraint introduces long-range dependencies between
the tag variables, complicating inference.
</p>

<ol class="problem">

<li class="writeup">
<strong>(2 points) Treewidth.</strong>
What is the treewidth of the CRF given above? Describe the associated variable elimination order.
</li>

<li class="writeup">
<strong>(2 points) Gibbs sampling for linear-chain CRFs I.</strong>
<p>
Before we look at a Gibbs sampler to handle long-range dependencies,
let's revisit the linear-chain CRF that we studied in problem
1. Recall from class that Gibbs sampling updates
   $y_t$ by sampling from its
   conditional distribution, given the values of the rest of the
   variables, $y_{-t} = (y_1, \dots, y_{t-1}, y_{t+1}, \dots, y_T)$.

<b>Write an expression for the conditional distribution $p(y_t \mid y_{-t},
  \xseq; \theta)$ for the linear-chain CRF in terms of the potentials $G_t$.</b>
</p>
</li>

<li class="code">
<strong>(5 points) Gibbs sampling for linear-chain CRFs II.</strong>
<p>
We have provided you a function <code>gibbsRun()</code>
which provides the framework for the Gibbs sampler
(see <code>submission.py</code> for documentation).
</p>

<p>
  <ul>
    <li>
<b>
      Implement <code>chooseGibbsCRF</code> that samples
      a value for $y_t$ based on its conditional distribution
      you derived above, and reassigns $y_t$ to that value.
</b>
      Note that you should only use the potential between $y_t$ and its Markov blanket.
    </li>
    <li>
<b>
      Implement <code>computeGibbsProbabilities</code> that
      estimates the probability for each output sequence based on the
      samples of the Gibbs sampler.
</b>
    </li>
    <li>
<b>
      Implement <code>computeGibbsBestSequence</code> that estimates the
      most likely sequence (the interface is similar to <code>computeViterbi</code>).
</b>
    </li>
  </ul>
</b>
</p>

<p>
Once you have implemented these functions, you can run the following
command(s) to look at the output of Gibbs.
<pre>
$ python run.py shell --parameters data/english.binary.crf --featureFunction binary
&gt;&gt; gibbs_best Mark had worked at Reuters
-PER-	-O-	-O-	-O-	-ORG-
&gt;&gt; gibbs_dist Mark had worked at Reuters
0.622 	-PER-	-O-	-O-	-O-	-ORG-
0.1902 	-PER-	-O-	-PER-	-O-	-ORG-
0.124 	-PER-	-O-	-ORG-	-O-	-ORG-
0.0321 	-ORG-	-O-	-O-	-O-	-ORG-
0.0084 	-ORG-	-O-	-PER-	-O-	-ORG-
</pre>
Your numbers will not match exactly due to the randomness in sampling.
</p>

</li>

<li class="writeup">
<strong>(2 points) Blocked Gibbs sampling I.</strong>
<p>
So far, we've used Gibbs sampling in a setting where we could have done exact inference.
But we now tackle the CRF with long-range dependencies, where the strength of Gibbs sampling will really shine.
In order to respect the hard constraint that the tags of identical words be
identical, we will modify the Gibbs sampler we learned in class to update
multiple variables at the same time.
</p>

<p>
Define a block to be a subset $S \subset \{ 1, \dots, T \}$ of the positions,
and let $y_S = (y_t)_{t \in S}$ be the set of variables in that block.
The observed words $\xseq = (x_1, \dots, x_T)$ induces a partitioning of the positions into blocks.
For example, for the sentence "A A B C B A C", we
have three blocks: $\{ \{1, 2, 6\}, \{3, 5\}, \{4, 7\} \}$.
Rather than sampling from the conditional distribution of a single variable $y_t$,
we will sample from the conditional distribution over all the variables in a block $S$:
$p(y_S \mid y_{-S}, \xseq; \theta)$.
</p>

<b>Write an expression for the conditional distribution $p(y_S \mid y_{-S}, \xseq; \theta)$
under the CRF with long-range dependencies.
Assume all the words $x_t$ for $t \in S$ are equal.</b>
</li>

<li class="code">
<strong>(4 points) Blocked Gibbs sampling.</strong>
<b>Implement <code>getLongRangeCRFBlocks</code> and <code>chooseGibbsLongRangeCRF</code></b>.
<p>
Once you have implemented these functions, you can run the following
command(s) to look at the output of Gibbs sampling under the regular linear-chain CRF:
<pre>
$ python run.py shell --parameters data/english.binary.crf --featureFunction binary
&gt;&gt; gibbs_dist Werner &amp; Co entered court today . Werner maintained that they were not guilty .
0.2819  -ORG- -ORG- -ORG- -O- -O- -O- -O- -PER- -O- -O- -O- -O- -O- -O- -O-
0.1063  -ORG- -ORG- -ORG- -O- -O- -O- -O- -PER- -PER- -O- -O- -O- -O- -O- -O-
0.0774  -ORG- -ORG- -ORG- -O- -O- -O- -O- -PER- -O- -O- -O- -O- -O- -PER- -O-
0.0697  -ORG- -ORG- -ORG- -O- -O- -PER- -O- -PER- -O- -O- -O- -O- -O- -O- -O-
0.0503  -ORG- -ORG- -ORG- -O- -O- -ORG- -O- -PER- -O- -O- -O- -O- -O- -O- -O-
</pre>
as well as under the CRF with long-range dependencies:
<pre>
$ python run.py shell --parameters data/english.binary.crf --featureFunction binary
&gt;&gt; lrgibbs_dist Werner &amp; Co entered court today . Werner maintained that they were not guilty .
0.8382  -ORG- -ORG- -ORG- -O- -O- -O- -O- -ORG- -O- -O- -O- -O- -O- -O- -O-
0.0506  -ORG- -ORG- -ORG- -O- -O- -PER- -O- -ORG- -O- -O- -O- -O- -O- -O- -O-
0.0427  -ORG- -ORG- -ORG- -O- -O- -O- -O- -ORG- -O- -O- -O- -O- -O- -PER- -O-
0.0266  -ORG- -ORG- -ORG- -O- -O- -ORG- -O- -ORG- -O- -O- -O- -O- -O- -O- -O-
0.017   -ORG- -ORG- -ORG- -O- -O- -O- -O- -ORG- -ORG- -O- -O- -O- -O- -O- -O-
</pre>
Note how the long-range dependencies help us get this example correct!
</p>

</li>

</ol>

</body>
